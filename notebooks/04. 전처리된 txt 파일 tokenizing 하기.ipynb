{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04. 전처리된 txt 파일 tokenizing 하기\n",
    "- 작성 : 정민정 (https://github.com/jeina7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "- 전처리 된 txt 파일을 `tokenization.py` 파일로 토크나이징 한다.\n",
    "\n",
    "\n",
    "- 토크나이징 된 numpy array는 `data.hdf5` 파일로 저장한다.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 저장된 데이터 `data.txt` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ufeff나는 어려서부터 노래를 좋아했다. 설거지를 하거나 청소를 하며 흥얼흥얼 노래를 하면 시간이 빨리 간다. 아무리 답답해도 노래를 부르고 나면 기분이 상쾌해진다. 슬플 때에도 큰 소리로 노래를 하면 목이 메지만 속은 후련해진다. 장소에 따라 겉으로 부를 때도 있고 속으로 부르기도 한다. 실은 속으로 부를 때가 더 많다. 결혼을 해서 얼마 되지 않아 서로가 서먹할 때이다. 불같으신 시어머니의 꾸중을 들었다. 어떻게든 분위기를 바꿔 보려고 소리 내지 않고 속으로 성가(聖歌) 한 곡을 부르고 나니 거짓말처럼 어머님의 노여움이 풀어지신 듯 했다. 그 후로 자녀들과 언짢은 일이 있을 때에 속으로 노래를 해 보는데 효과가 있었다. 그러다 보니 속으로 부르는 노래가 저절로 습관이 되었다. 새벽부터 밤중까지 내가 부르는 노래는 수없이 많다. 사람들과 일상적인 이야기를 나누면서도 머리 속에는 항상 리듬이 흐른다. 속으로 부르는 노래는 내게 있어서 기도일 수도 있고, 내 삶의 버팀목이 되기도 한다. 성'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = './Crawling/textcrawler/use_data/essay.txt'\n",
    "\n",
    "with open(file_path, 'r') as f:\n",
    "    data = f.read()\n",
    "\n",
    "data[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 3. 단어 사전 `vocab.txt` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.src.tokenization import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all tokens: 112074 \n",
      "\n",
      "[PAD]: 0\n",
      "[UNK]: 1\n",
      "[CLS]: 2\n",
      "[SEP]: 3\n",
      "[MASK]: 4\n",
      "Lyon: 5\n",
      "유리수: 6\n",
      "두호: 7\n",
      "##해온: 8\n",
      "나락: 9\n",
      "볼모: 10\n",
      "##정기예금: 11\n",
      "출조: 12\n",
      "##디자이너: 13\n",
      "호이안: 14\n",
      "##adintext: 15\n",
      "##습의: 16\n",
      "토왕: 17\n",
      "##통역장교: 18\n",
      "##초항: 19\n"
     ]
    }
   ],
   "source": [
    "vocab_path = \"./common/models/345K/vocab.txt\"\n",
    "vocab = load_vocab(vocab_path)\n",
    "print('all tokens:', len(vocab), '\\n')\n",
    "\n",
    "cnt = 1\n",
    "for k, v in vocab.items():\n",
    "    print(\"{}: {}\".format(k, v))\n",
    "    if cnt == 20: break\n",
    "    cnt += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `vocab.txt`는 모든 단어들의 id가 담겨있다\n",
    "\n",
    "\n",
    "- 이걸 활용해서 모든 token들을 id로 변환함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2. plain text를 전처리 할 `FullTokenizer` class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 전처리 과정을 test 해보기 위해 300자를 자른 sample text를 활용해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ufeff나는 어려서부터 노래를 좋아했다. 설거지를 하거나 청소를 하며 흥얼흥얼 노래를 하면 시간이 빨리 간다. 아무리 답답해도 노래를 부르고 나면 기분이 상쾌해진다. 슬플 때에도 큰 소리로 노래를 하면 목이 메지만 속은 후련해진다. 장소에 따라 겉으로 부를 때도 있고 속으로 부르기도 한다. 실은 속으로 부를 때가 더 많다. 결혼을 해서 얼마 되지 않아 서로가 서먹할 때이다. 불같으신 시어머니의 꾸중을 들었다. 어떻게든 분위기를 바꿔 보려고 소리 내지 않고 속으로 성가(聖歌) 한 곡을 부르고 나니 거짓말처럼 어머님의 노여움이 풀어지신 듯 '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = data[:300]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['나', '##는', '어려서', '##부터', '노래', '##를', '좋', '##아', '##했다', '.', '설거지', '##를', '하', '##거나', '청소', '##를', '하', '##며', '흥', '##얼', '##흥', '##얼', '노래', '##를', '하', '##면', '시간', '##이', '빨리', '간다', '.', '아무리', '답답', '##해도', '노래', '##를', '부르', '##고', '나', '##면', '기분', '##이', '상쾌', '##해진다', '.', '슬', '##플', '때', '##에', '##도', '큰', '소리', '##로', '노래', '##를', '하', '##면', '목이', '메', '##지만', '속', '##은', '후련', '##해진다', '.', '장소', '##에', '따라', '겉', '##으로', '부', '##를', '때', '##도', '있', '##고', '속', '##으로', '부르', '##기도', '한다', '.', '실은', '속', '##으로', '부', '##를', '때', '##가', '더', '많', '##다', '.', '결혼', '##을', '해서', '얼마', '되', '##지', '않아', '서로', '##가', '서', '##먹', '##할', '때', '##이', '##다', '.', '불', '##같', '##으신', '시어머니', '##의', '꾸', '##중', '##을', '들', '##었', '##다', '.', '어떻게', '##든', '분위기', '##를', '바꿔', '보', '##려고', '소리', '내지', '않', '##고', '속', '##으로', '성가', '(', '[UNK]', '[UNK]', ')', '한', '곡', '##을', '부르', '##고', '나', '##니', '거짓말', '##처럼', '어머님', '##의', '노', '##여', '##움', '##이', '풀', '##어지', '##신', '듯']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = FullTokenizer(vocab_file=vocab_path, do_lower_case=False)\n",
    "\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 모든 단어가 형태소 단위로 쪼개졌음을 확인할 수 있다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 4. token to ids 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12021, 66682, 71592, 11141, 31928, 91451, 16710, 100851, 4582, 59230, 99051, 91451, 86690, 108833, 82915, 91451, 86690, 55635, 42470, 37837, 34186, 37837, 31928, 91451, 86690, 13773, 103593, 54917, 42030, 56607, 59230, 63167, 67186, 102338, 31928, 91451, 40497, 47248, 12021, 13773, 13257, 54917, 66236, 24826, 59230, 74408, 11941, 100562, 38485, 91440, 90877, 2384, 110598, 31928, 91451, 86690, 13773, 99784, 6917, 90269, 69791, 102004, 27397, 24826, 59230, 78382, 38485, 89434, 16649, 63550, 28667, 91451, 100562, 91440, 2051, 47248, 69791, 63550, 40497, 66351, 60146, 59230, 56358, 69791, 63550, 28667, 91451, 100562, 41251, 106837, 86314, 22935, 59230, 68434, 69343, 28571, 79154, 41520, 46190, 43196, 73928, 41251, 61635, 23191, 101461, 100562, 54917, 22935, 59230, 37331, 37576, 88316, 56350, 35102, 15095, 26161, 69343, 51571, 43616, 22935, 59230, 103826, 19682, 107948, 91451, 25546, 78972, 35845, 2384, 55757, 94313, 47248, 69791, 63550, 66610, 40947, 1, 1, 51193, 108235, 35676, 69343, 40497, 47248, 12021, 109958, 33372, 35235, 81214, 35102, 11627, 92689, 51751, 54917, 38341, 5249, 95234, 35561]\n"
     ]
    }
   ],
   "source": [
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위에서 본 모든 단어 토큰들이 id로 변환됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 5. 전체 텍스트 데이터를 id로 변환하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get text data file\n",
    "def get_tokenized_data(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = f.read()\n",
    "\n",
    "    # Tokenizing\n",
    "    tokenizer = FullTokenizer(vocab_file=vocab_path, do_lower_case=False)\n",
    "    print(\"Tokenizing all data...\")\n",
    "    tokens = tokenizer.tokenize(data)\n",
    "    data_token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    print(\"Done.\")\n",
    "    \n",
    "    return data_token_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 약 20Mb 의 데이터를 tokenizing 하고, 그걸 다시 id로 변환하는 작업\n",
    "\n",
    "\n",
    "- 위 작업은 약 1분 정도 소요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 6. 변환된 `data_token_ids`를 `hdf5` 파일로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing all data...\n",
      "Done.\n",
      "Tokenizing all data...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# save as hdf5 file\n",
    "file_1 = './Crawling/textcrawler/use_data/essay.txt'\n",
    "file_2 = './Crawling/textcrawler/use_data/removed_sum.txt'\n",
    "save_path = './common/data/crawled_data_2.hdf5'\n",
    "\n",
    "data_token_ids_1 = get_tokenized_data(file_1)\n",
    "data_token_ids_2 = get_tokenized_data(file_2)\n",
    "\n",
    "f = h5py.File(save_path, 'w')\n",
    "g = f.create_group(\"data\")\n",
    "g.create_dataset(\"sub_data_1\", data=data_token_ids_1)\n",
    "g.create_dataset(\"sub_data_2\", data=data_token_ids_2)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `data.hdf5` 명으로 데이터 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = './common/data/data.hdf5'\n",
    "\n",
    "f = h5py.File(save_path, 'w')\n",
    "g = f.create_group(\"data\")\n",
    "g.create_dataset(\"crawled_data\", data=token_ids)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 저장된 데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 file \"data.hdf5\" (mode r)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fi = h5py.File(save_path, 'r')\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KeysViewHDF5 ['data']>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fi.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"crawled_geulteen\": shape (14548850,), type \"<i8\">"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fi['data']['crawled_geulteen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 32506,  64805,  10739,  37723,  56104,  69343, 101325, 100851,\n",
       "        92802,  46304, 106505,  90637,    247,  22573,  38265,  12255,\n",
       "       104236,  63873,  11244,  54917,  63873,  25616,  85189,  69343,\n",
       "        41244,  18166,  37980,  36266,  55635,  37544,  30178,  41251,\n",
       "        50535,  69343,  36058,  59230,  12021,  66682,  39803, 108833,\n",
       "        38265, 108833,  12694,  26719,  48502,  46190,  94313,  70242,\n",
       "        59230,  96141,  44426,  66101,   7286,  74951,  38485,  51266,\n",
       "       104907,  54917,  86314, 100851,  84263,  97360,  91440,  75291,\n",
       "        68324,  59230,  37544,  30178,  66682,  38110, 101743,  10739,\n",
       "        95738,  43717, 111844,  59230,  82805,  12559,  91440,   4991,\n",
       "        43616, 102258,   8966,  41251,  57686,  22573,  65141, 106126,\n",
       "        46190,   3442,  15996,   2384,  91451,  55253,  13773,  38110,\n",
       "        74951,  54917,  17299,  79701])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fi['data']['crawled_geulteen'][:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 14,548,850 길이의 데이터 확인"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
